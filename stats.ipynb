{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#28abaf','#66ccff','#EE0000','#006666','#FFFF00','#0080FF']\n",
    "import matplotlib.pyplot as plt\n",
    "p = '/home/kent/fonts/Roboto_Condensed/static'\n",
    "import os\n",
    "all_font_flie_names = os.listdir(p)\n",
    "all_ttf_file_names = [n for n in all_font_flie_names if n.endswith('.ttf')]\n",
    "from matplotlib.font_manager import fontManager\n",
    "for ttf_file_name in all_ttf_file_names:\n",
    "    fontManager.addfont(path=os.path.join(p,ttf_file_name))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = ['sans-serif'] \n",
    "plt.rcParams['font.sans-serif'] = ['Roboto Condensed'] \n",
    "plt.rcParams['font.size'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import IDEA_MODELS, CRITIC_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_model_name(name):\n",
    "    # Process the path, remove the part before the \"/\"\n",
    "    if '/' in name:\n",
    "        name = name.split('/', 1)[1]  # Split only once, take the second part\n",
    "    \n",
    "    # Process specific suffixes\n",
    "    if ':' in name:\n",
    "        base_name, suffix = name.split(':', 1)\n",
    "        # Only remove the suffix when it is a specific type\n",
    "        if suffix in ['Q4_K_M', 'free']:\n",
    "            return base_name\n",
    "        # Otherwise, keep the full name, including the colon and suffix\n",
    "        return name\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./csvs/view.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del all rows with anthropic/claude-3-haiku\n",
    "# df = df[df['idea_model'] != 'anthropic/claude-3-haiku']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the average length of each idea_model's ideas, and sort them in descending order\n",
    "df['idea_length_in_char'] = df['idea'].apply(lambda x: len(str(x)))\n",
    "# Only ideas with less than 2500 characters are considered valid\n",
    "df = df[df['idea_length_in_char'] < 2500]\n",
    "# Only ideas with more than 100 characters are considered valid\n",
    "df = df[df['idea_length_in_char'] > 100]\n",
    "\n",
    "df['idea_length_in_words'] = df['idea'].apply(lambda x: len(str(x).split()))\n",
    "df['idea_length_in_words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If './figs/' does not exist, create it; if it exists, ignore it\n",
    "os.makedirs('./figs/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to ideas with < 200 words\n",
    "df = df[df['idea_length_in_words'] < 200]\n",
    "# View histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.hist(df['idea_length_in_words'], bins=100, color='#28abaf')\n",
    "plt.xlabel('idea length in words')\n",
    "plt.ylabel('number of ideas')\n",
    "plt.tight_layout() # Add this line\n",
    "plt.savefig('figs/idea_length_in_words.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many unique keywords there are\n",
    "n = df['keywords'].nunique()\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which keyword (kw) and which model have not been tested\n",
    "agg = df.groupby(['keywords', 'idea_model']).size()\n",
    "print(agg)\n",
    "# print(results_df.groupby(['kw', 'idea_model']).size())\n",
    "\n",
    "last_run_index = agg.values.max()\n",
    "print(last_run_index)\n",
    "print('Current maximum count:', last_run_index)\n",
    "# Get the current minimum count\n",
    "last_run_index = agg.values.min()\n",
    "print(last_run_index)\n",
    "print('Current minimum count (needs to be supplemented):', last_run_index)\n",
    "\n",
    "# Count how many models have the same count as the minimum count\n",
    "print(agg.values.min()) \n",
    "print(agg.values.min() == agg.values)\n",
    "print(sum(agg.values.min() == agg.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(agg.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = df['raw_critique'].tolist()\n",
    "print(string[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = eval(df['parsed_scores'].iloc[0])\n",
    "print(score_dict)\n",
    "score_dict['originality']\n",
    "score_dict['feasibility']\n",
    "score_dict['clarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that returns None if the evaluation fails\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return eval(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply the function and remove rows where the result is None\n",
    "df['scores'] = df['parsed_scores'].apply(safe_eval)\n",
    "df = df.dropna(subset=['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['originality'] = df['scores'].apply(lambda x: x['originality'])\n",
    "df['feasibility'] = df['scores'].apply(lambda x: x['feasibility'])\n",
    "df['clarity'] = df['scores'].apply(lambda x: x['clarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to check if a scores dictionary contains any 'N/A' values\n",
    "def has_na(scores_dict):\n",
    "    return any(val == 'N/A' for val in scores_dict.values())\n",
    "\n",
    "# Filter the dataframe to remove rows with 'N/A' scores\n",
    "df = df[~df['scores'].apply(has_na)]\n",
    "\n",
    "# Now calculate mean and min on the filtered dataframe\n",
    "df['mean_score'] = df['scores'].apply(lambda x: sum(float(val) for val in x.values()) / len(x) if len(x) != 0 else 0)\n",
    "df['min_score'] = df['scores'].apply(lambda x: min(float(val) for val in x.values()) if len(x) != 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.groupby('idea_model')['idea_length_in_words'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model with the highest mean_score, sort in descending order\n",
    "# df.groupby('idea_model')['mean_score'].mean().sort_values(ascending=False)\n",
    "\n",
    "df_view = df.groupby('idea_model').agg({'mean_score':['mean','std','count'],'idea_length_in_words':['mean','std','count']}).sort_values(('mean_score','mean'),ascending=False)\n",
    "# df_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_confidence_interval(mean, std, count, confidence=0.95):\n",
    "    sem = std / np.sqrt(count)\n",
    "    ci = stats.t.interval(confidence, count-1, loc=mean, scale=sem)\n",
    "    return ci\n",
    "\n",
    "# Assuming your dataframe is named df\n",
    "results = []\n",
    "\n",
    "# Directly using the values from the DataFrame\n",
    "score_means = df_view[('mean_score', 'mean')]\n",
    "score_stds = df_view[('mean_score', 'std')]\n",
    "score_counts = df_view[('mean_score', 'count')]\n",
    "length_means = df_view[('idea_length_in_words', 'mean')]\n",
    "length_stds = df_view[('idea_length_in_words', 'std')]\n",
    "length_counts = df_view[('idea_length_in_words', 'count')]\n",
    "\n",
    "for i in range(len(df_view)):\n",
    "    # Calculating the confidence interval\n",
    "    score_ci = calculate_confidence_interval(\n",
    "        score_means.iloc[i],\n",
    "        score_stds.iloc[i],\n",
    "        score_counts.iloc[i]\n",
    "    )\n",
    "    \n",
    "    length_ci = calculate_confidence_interval(\n",
    "        length_means.iloc[i],\n",
    "        length_stds.iloc[i],\n",
    "        length_counts.iloc[i]\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'model': df_view.index[i],\n",
    "        'score_mean': score_means.iloc[i],\n",
    "        'score_ci_lower': score_ci[0],\n",
    "        'score_ci_upper': score_ci[1],\n",
    "        'score_ci_range': score_ci[1] - score_ci[0],\n",
    "        'length_mean': length_means.iloc[i],\n",
    "        'length_ci_lower': length_ci[0],\n",
    "        'length_ci_upper': length_ci[1],\n",
    "        'length_ci_range': length_ci[1] - length_ci[0]\n",
    "    })\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "# Setting the display format\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "# Set color palette\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "scale = 0.8\n",
    "\n",
    "# --- Part 1: Prepare data ---\n",
    "# Assuming the df already has these three fields:\n",
    "# df['originality'] = df['scores'].apply(lambda x: x['originality'])\n",
    "# df['feasibility'] = df['scores'].apply(lambda x: x['feasibility'])\n",
    "# df['clarity'] = df['scores'].apply(lambda x: x['clarity'])\n",
    "\n",
    "# 1. Calculate mean and confidence interval\n",
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    n = len(data)\n",
    "    sem = std / np.sqrt(n)\n",
    "    ci = stats.t.interval(confidence, n-1, loc=mean, scale=sem)\n",
    "    return mean, ci[0], ci[1], ci[1] - ci[0]\n",
    "\n",
    "# Calculate statistics by model\n",
    "result_data = []\n",
    "for model_name, group in df.groupby('idea_model'):\n",
    "    # Calculate mean and confidence interval for all metrics\n",
    "    orig_mean, orig_ci_lower, orig_ci_upper, orig_ci_range = calculate_confidence_interval(group['originality'])\n",
    "    feas_mean, feas_ci_lower, feas_ci_upper, feas_ci_range = calculate_confidence_interval(group['feasibility'])\n",
    "    clar_mean, clar_ci_lower, clar_ci_upper, clar_ci_range = calculate_confidence_interval(group['clarity'])\n",
    "    \n",
    "    # Calculate the overall score (average of the three metrics)\n",
    "    score_mean = (orig_mean + feas_mean + clar_mean) / 3\n",
    "    # Calculate the confidence interval for the overall score (simplified)\n",
    "    score_ci_lower = (orig_ci_lower + feas_ci_lower + clar_ci_lower) / 3\n",
    "    score_ci_upper = (orig_ci_upper + feas_ci_upper + clar_ci_upper) / 3\n",
    "    score_ci_range = score_ci_upper - score_ci_lower\n",
    "    \n",
    "    # Calculate length statistics\n",
    "    length_mean, length_ci_lower, length_ci_upper, length_ci_range = calculate_confidence_interval(group['idea_length_in_words'])\n",
    "    \n",
    "    result_data.append({\n",
    "        'idea_model': model_name,\n",
    "        'score_mean': score_mean,\n",
    "        'score_ci_lower': score_ci_lower,\n",
    "        'score_ci_upper': score_ci_upper,\n",
    "        'score_ci_range': score_ci_range,\n",
    "        'length_mean': length_mean,\n",
    "        'length_ci_lower': length_ci_lower,\n",
    "        'length_ci_upper': length_ci_upper,\n",
    "        'length_ci_range': length_ci_range,\n",
    "        'originality_mean': orig_mean,\n",
    "        'originality_ci_lower': orig_ci_lower,\n",
    "        'originality_ci_upper': orig_ci_upper,\n",
    "        'originality_ci_range': orig_ci_range,\n",
    "        'feasibility_mean': feas_mean,\n",
    "        'feasibility_ci_lower': feas_ci_lower,\n",
    "        'feasibility_ci_upper': feas_ci_upper,\n",
    "        'feasibility_ci_range': feas_ci_range,\n",
    "        'clarity_mean': clar_mean,\n",
    "        'clarity_ci_lower': clar_ci_lower,\n",
    "        'clarity_ci_upper': clar_ci_upper,\n",
    "        'clarity_ci_range': clar_ci_range\n",
    "    })\n",
    "\n",
    "result_df = pd.DataFrame(result_data)\n",
    "\n",
    "scatter_size = 30\n",
    "\n",
    "# Simplify model names\n",
    "result_df['short_name'] = result_df['idea_model'].apply(lambda x: clean_model_name(x))\n",
    "\n",
    "# Calculate y-axis label font size - the default font size is typically 10, we take 80%\n",
    "y_label_fontsize = 11  # 10 * 0.8 = 8\n",
    "\n",
    "# --- First plot: two subplots arranged horizontally ---\n",
    "fig1, (ax1, ax2) = plt.subplots(1, 2, figsize=(16*scale, 9*scale))  # Arrange 2 subplots horizontally\n",
    "\n",
    "# Sort by score_mean in descending order\n",
    "sorted_df = result_df.sort_values('score_mean', ascending=True)\n",
    "\n",
    "# Create y-coordinates\n",
    "y_pos = np.arange(len(sorted_df))\n",
    "\n",
    "# Update colors: use #52ccc3 for average score\n",
    "avg_score_color = '#52ccc3'\n",
    "length_color = 'grey'  # Keep length color unchanged\n",
    "\n",
    "# Plot score chart - left subplot\n",
    "ax1.scatter(sorted_df['score_mean'], y_pos, color=avg_score_color, s=scatter_size, zorder=3)\n",
    "\n",
    "# Add rounded error bars and value labels\n",
    "for i, row in enumerate(sorted_df.itertuples()):\n",
    "    ax1.hlines(y=i, xmin=row.score_ci_lower, xmax=row.score_ci_upper,\n",
    "              color=avg_score_color, alpha=0.9, linewidth=3.0, capstyle='round')\n",
    "    # Add label text with white border effect\n",
    "    label_text = f\"{row.score_mean:.2f}±{row.score_ci_range/2:.2f}\"\n",
    "    text = ax1.text(row.score_ci_upper + 0.05, i, label_text, \n",
    "            va='center', ha='left', fontsize=9, color=avg_score_color)\n",
    "    text.set_path_effects([path_effects.withStroke(linewidth=4, foreground='white')])\n",
    "\n",
    "# Set y-axis tick labels\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(sorted_df['short_name'], fontsize=y_label_fontsize)  # Modify here\n",
    "\n",
    "# Set title and labels\n",
    "ax1.set_title('Idea Score Avg. by Model: $\\\\frac{1}{3}$ (Orig. + Feas. + Clar.)')\n",
    "ax1.set_xlabel('Score')\n",
    "# Remove \"Model\" label\n",
    "# ax1.set_ylabel('Model')\n",
    "\n",
    "# Set grid lines\n",
    "ax1.grid(True, axis='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust x-axis range\n",
    "x_max = sorted_df['score_ci_upper'].max() + 0.3\n",
    "ax1.set_xlim(right=x_max)\n",
    "\n",
    "# Keep only left and bottom border lines\n",
    "ax1.spines['top'].set_visible(False)\n",
    "ax1.spines['right'].set_visible(False)\n",
    "\n",
    "# Plot length chart - right subplot\n",
    "ax2.scatter(sorted_df['length_mean'], y_pos, color=length_color, s=scatter_size, zorder=3)\n",
    "\n",
    "# Add rounded error bars and value labels\n",
    "for i, row in enumerate(sorted_df.itertuples()):\n",
    "    ax2.hlines(y=i, xmin=row.length_ci_lower, xmax=row.length_ci_upper,\n",
    "              color=length_color, alpha=0.9, linewidth=5.0, capstyle='round')\n",
    "    # Add label text with white border effect\n",
    "    label_text = f\"{row.length_mean:.2f}±{row.length_ci_range/2:.2f}\"\n",
    "    text = ax2.text(row.length_ci_upper + 5, i, label_text, \n",
    "            va='center', ha='left', fontsize=9, color=length_color)\n",
    "    text.set_path_effects([path_effects.withStroke(linewidth=4, foreground='white')])\n",
    "\n",
    "# Set y-axis tick labels\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(sorted_df['short_name'], fontsize=y_label_fontsize)  # Modify here\n",
    "\n",
    "# Set title and labels\n",
    "ax2.set_title('Idea Length Avg. by Model')\n",
    "ax2.set_xlabel('Length (Words)')\n",
    "# Remove \"Model\" label\n",
    "# ax2.set_ylabel('Model')\n",
    "\n",
    "# Set grid lines\n",
    "ax2.grid(True, axis='both', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust x-axis range\n",
    "x_max = sorted_df['length_ci_upper'].max() + 30\n",
    "ax2.set_xlim(right=x_max)\n",
    "\n",
    "# Keep only left and bottom border lines\n",
    "ax2.spines['top'].set_visible(False)\n",
    "ax2.spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/mean_score_and_length_rank.pdf')\n",
    "plt.show()\n",
    "\n",
    "# --- Second plot: three subplots arranged horizontally ---\n",
    "fig2, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20*scale, 9*scale))  # Arrange 3 subplots horizontally\n",
    "\n",
    "# Update color settings\n",
    "metrics = [\n",
    "    {'name': 'originality', 'color': '#66ccff', 'title': 'Originality Score'},  # Use new color\n",
    "    {'name': 'feasibility', 'color': '#5092e4', 'title': 'Feasibility Score'},  # Use new color\n",
    "    {'name': 'clarity', 'color': '#ffa500', 'title': 'Clarity Score'}  # Use new color\n",
    "]\n",
    "\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_name = metric['name']\n",
    "    \n",
    "    # Sort by the current metric in descending order\n",
    "    sorted_df = result_df.sort_values(f'{metric_name}_mean', ascending=True)\n",
    "    \n",
    "    # Create y-coordinates\n",
    "    y_pos = np.arange(len(sorted_df))\n",
    "    \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot scatter\n",
    "    ax.scatter(sorted_df[f'{metric_name}_mean'], y_pos, \n",
    "              color=metric['color'], s=scatter_size, zorder=3)\n",
    "    \n",
    "    # Add rounded error bars and value labels\n",
    "    for j, row in enumerate(sorted_df.itertuples()):\n",
    "        mean_val = getattr(row, f'{metric_name}_mean')\n",
    "        ci_lower = getattr(row, f'{metric_name}_ci_lower')\n",
    "        ci_upper = getattr(row, f'{metric_name}_ci_upper')\n",
    "        ci_range = getattr(row, f'{metric_name}_ci_range')\n",
    "        \n",
    "        ax.hlines(y=j, xmin=ci_lower, xmax=ci_upper,\n",
    "                color=metric['color'], alpha=0.9, linewidth=3.0, capstyle='round')\n",
    "        \n",
    "        # Add label text with white border effect\n",
    "        label_text = f\"{mean_val:.2f}±{ci_range/2:.2f}\"\n",
    "        text = ax.text(ci_upper + 0.05, j, label_text, \n",
    "               va='center', ha='left', fontsize=9, color=metric['color'])\n",
    "        text.set_path_effects([path_effects.withStroke(linewidth=4, foreground='white')])\n",
    "    \n",
    "    # Set y-axis tick labels\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(sorted_df['short_name'], fontsize=y_label_fontsize)  # Modify here\n",
    "    \n",
    "    # Set title and labels\n",
    "    ax.set_title(f'LiveIdeaBench {metric[\"title\"]}')\n",
    "    ax.set_xlabel('Score')\n",
    "\n",
    "    # Adjust x-axis range\n",
    "    x_max = sorted_df[f'{metric_name}_ci_upper'].max() + 0.5\n",
    "    ax.set_xlim(right=x_max)\n",
    "    \n",
    "    # Set grid lines\n",
    "    ax.grid(True, axis='both', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Keep only left and bottom border lines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/originality_feasibility_clarity_scores.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Create a detailed table for the three metrics\n",
    "detailed_cols = ['short_name']\n",
    "for metric in ['originality', 'feasibility', 'clarity']:\n",
    "    detailed_cols.extend([f'{metric}_mean', f'{metric}_ci_range'])\n",
    "\n",
    "detailed_df = result_df[detailed_cols].sort_values('originality_mean', ascending=False)\n",
    "print(\"\\nDetailed Metrics Table (sorted by originality_mean):\")\n",
    "print(detailed_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Set the canvas and grid - further reduce the width of the color axis\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(4, 5, width_ratios=[0.3, 1.6, 1.6, 1.6, 1])  # Reduce the width of the left color axis\n",
    "\n",
    "# Create the main plot and marginal distribution plots\n",
    "ax_main = plt.subplot(gs[1:4, 1:4])  # Main plot\n",
    "ax_x = plt.subplot(gs[0, 1:4])       # X-axis marginal distribution (top)\n",
    "ax_y = plt.subplot(gs[1:4, 4])       # Y-axis marginal distribution (right)\n",
    "cbar_ax = plt.subplot(gs[1:4, 0])    # Color axis (left, narrower)\n",
    "\n",
    "# Plot the hexbin on the main plot\n",
    "hb = ax_main.hexbin(df['idea_length_in_words'], df['mean_score'], \n",
    "                   gridsize=30, cmap='Blues', mincnt=1, bins='log')\n",
    "\n",
    "# Draw the regression line\n",
    "sns.regplot(x='idea_length_in_words', y='mean_score', data=df,\n",
    "            scatter=False, color='crimson', line_kws={'linewidth': 2}, ax=ax_main)\n",
    "\n",
    "# Calculate the correlation coefficient and p-value\n",
    "corr, p_value = stats.pearsonr(df['idea_length_in_words'], df['mean_score'])\n",
    "\n",
    "# Determine the text to display based on the p-value\n",
    "if p_value < 0.0001:\n",
    "    p_text = 'P < 0.0001'\n",
    "elif p_value < 0.001:\n",
    "    p_text = 'P < 0.001'\n",
    "elif p_value < 0.01:\n",
    "    p_text = 'P < 0.01'\n",
    "elif p_value < 0.05:\n",
    "    p_text = 'P < 0.05'\n",
    "else:\n",
    "    p_text = f'P = {p_value:.3f}'\n",
    "    \n",
    "# Add the correlation coefficient information - increase the font size\n",
    "ax_main.text(0.95, 0.95, f'r = {corr:.3f}\\n{p_text}',\n",
    "             horizontalalignment='right',\n",
    "             verticalalignment='top',\n",
    "             transform=ax_main.transAxes,\n",
    "             fontsize=14,  # Increase the font size\n",
    "             bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', boxstyle='round,pad=0.5'))\n",
    "\n",
    "# Plot the X-axis marginal distribution (histogram)\n",
    "sns.histplot(df['idea_length_in_words'], ax=ax_x, kde=False, color='#3484bf', alpha=0.7, \n",
    "             stat='density', linewidth=0, binwidth=5)\n",
    "ax_x.set_xlim(0, 200)\n",
    "ax_x.set_ylabel('Density', fontsize=12)\n",
    "ax_x.spines['top'].set_visible(False)\n",
    "ax_x.spines['right'].set_visible(False)\n",
    "ax_x.set_xlabel('')  # Remove the x-axis label as the main plot already has one\n",
    "ax_x.tick_params(axis='x', labelbottom=False)  # Remove the x-axis tick labels\n",
    "\n",
    "# Set the minor ticks for the X-axis histogram\n",
    "ax_x.xaxis.set_major_locator(ticker.MultipleLocator(25))  # Major ticks at multiples of 25\n",
    "ax_x.xaxis.set_minor_locator(ticker.MultipleLocator(5))   # Minor ticks at multiples of 5 (consistent with binwidth)\n",
    "\n",
    "# Plot the Y-axis marginal distribution (histogram)\n",
    "sns.histplot(y=df['mean_score'], ax=ax_y, kde=False, color='#3484bf', alpha=0.7, \n",
    "             stat='density', linewidth=0, binwidth=1/3)\n",
    "ax_y.set_ylim(ax_main.get_ylim())  # Keep the y-axis range consistent with the main plot\n",
    "ax_y.set_xlabel('Density', fontsize=12)\n",
    "ax_y.spines['top'].set_visible(False)\n",
    "ax_y.spines['right'].set_visible(False)\n",
    "ax_y.set_ylabel('')  # Remove the y-axis label as the main plot already has one\n",
    "ax_y.tick_params(axis='y', labelleft=False)  # Remove the y-axis tick labels\n",
    "\n",
    "# Set the minor ticks for the Y-axis histogram\n",
    "ax_y.yaxis.set_major_locator(ticker.MultipleLocator(2))   # Major ticks at multiples of 2\n",
    "ax_y.yaxis.set_minor_locator(ticker.MultipleLocator(1/3)) # Minor ticks at multiples of 1/3 (consistent with binwidth)\n",
    "\n",
    "# Beautify the main plot and set the ticks\n",
    "ax_main.spines['top'].set_visible(False)\n",
    "ax_main.spines['right'].set_visible(False)\n",
    "ax_main.set_xlim(0, 200)\n",
    "\n",
    "# Synchronize the ticks of the main plot\n",
    "ax_main.xaxis.set_major_locator(ticker.MultipleLocator(25))  # Major ticks at multiples of 25\n",
    "ax_main.xaxis.set_minor_locator(ticker.MultipleLocator(5))   # Minor ticks at multiples of 5\n",
    "ax_main.yaxis.set_major_locator(ticker.MultipleLocator(2))   # Major ticks at multiples of 2\n",
    "ax_main.yaxis.set_minor_locator(ticker.MultipleLocator(1/3)) # Minor ticks at multiples of 1/3\n",
    "\n",
    "# Set the title and labels\n",
    "ax_main.set_xlabel('Idea Length (words)', fontsize=14)\n",
    "ax_main.set_ylabel('Mean Score of Each Idea, $\\\\frac{1}{3}$ (Orig.+Feas.+Clar.)', fontsize=14)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Add the color bar on the left - narrower style\n",
    "cbar = plt.colorbar(hb, cax=cbar_ax)\n",
    "cbar.set_label('log(count)', fontsize=12)\n",
    "cbar_ax.yaxis.set_label_position('left')  # Place the label on the left side\n",
    "cbar_ax.yaxis.tick_left()  # Place the ticks on the left side\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/idea_length_mean_score_corr.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keywords = df['keywords'].unique().tolist()\n",
    "all_idea_models = df['idea_model'].unique().tolist()\n",
    "# all_keywords\n",
    "all_idea_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas.core.common as com\n",
    "\n",
    "# Creating a DataFrame containing the models and their thresholds\n",
    "thresholds = []\n",
    "for model in all_idea_models:\n",
    "    df_view = df[df['idea_model'] == model]\n",
    "    threshold = df_view['mean_score'].quantile(0.3)\n",
    "    # Extract the last part of the model name as a shorter display name\n",
    "    short_name = model.split('/')[-1]\n",
    "    thresholds.append({'model': short_name, 'threshold': threshold})\n",
    "\n",
    "df_thresholds = pd.DataFrame(thresholds)\n",
    "# Sort the thresholds in descending order\n",
    "df_thresholds = df_thresholds.sort_values('threshold', ascending=True)\n",
    "\n",
    "# Create the chart\n",
    "plt.figure(figsize=(12, 5))\n",
    "bars = plt.barh(df_thresholds['model'], df_thresholds['threshold'])\n",
    "plt.xlabel('30th Percentile Score')\n",
    "plt.title('Model Performance Comparison (30th Percentile)')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, \n",
    "             f'{df_thresholds[\"threshold\"].iloc[i]:.1f}', \n",
    "             va='center')\n",
    "\n",
    "# Adjust the layout to avoid label truncation\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_view = df[:].groupby(['idea_model','idea','keywords']).agg({'originality':['mean'],'feasibility':['mean'],'clarity':['mean'],'mean_score':['mean'],'critic_model':['count']}).sort_values(('mean_score','mean'),ascending=False)\n",
    "df_view = df_view[df_view[('critic_model','count')] == 3]\n",
    "display(df_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df.to_parquet('./data/data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型拒绝率统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_reject_rate = df['first_was_rejected'].sum() / len(df) * 100\n",
    "print('The Overall rejection rate is {:.4f}%'.format(overall_reject_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to mark true rejections\n",
    "df['true_rejection'] = (df['first_was_rejected'] == True) & (df['first_reject_response'].str.len() > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['true_rejection']][['keywords','idea_model','first_reject_response']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples for each model\n",
    "model_counts = df['idea_model'].value_counts()\n",
    "\n",
    "# Calculate the number of samples that were truly rejected for each model\n",
    "true_rejected_counts = df[df['true_rejection'] == True]['idea_model'].value_counts()\n",
    "\n",
    "# Calculate the true rejection rates\n",
    "true_rejection_rates = (true_rejected_counts / model_counts * 100).sort_values(ascending=False)\n",
    "\n",
    "# Create a DataFrame containing the model name, total samples, rejected samples, and rejection rate\n",
    "true_rejection_stats = pd.DataFrame({\n",
    "    'Total Samples': model_counts,\n",
    "    'Truly Rejected Samples': true_rejected_counts,\n",
    "    'True Rejection Rate (%)': true_rejection_rates.round(2)\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by true rejection rate in descending order\n",
    "true_rejection_stats = true_rejection_stats.sort_values('True Rejection Rate (%)', ascending=False)\n",
    "\n",
    "# Display the rejection rate statistics\n",
    "print(\"Model Rejection Rate Ranking:\")\n",
    "display(true_rejection_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total samples per model\n",
    "model_counts = df['idea_model'].value_counts()\n",
    "\n",
    "# Calculate truly rejected samples per model\n",
    "true_rejected_counts = df[df['true_rejection'] == True]['idea_model'].value_counts()\n",
    "\n",
    "# Calculate true rejection rates\n",
    "true_rejection_rates = (true_rejected_counts / model_counts * 100).sort_values(ascending=False)\n",
    "\n",
    "# Create a DataFrame with model stats\n",
    "rejection_stats = pd.DataFrame({\n",
    "    'Total Samples': model_counts,\n",
    "    'Rejected Samples': true_rejected_counts,\n",
    "    'Rejection Rate (%)': true_rejection_rates.round(2)\n",
    "})\n",
    "\n",
    "# Sort by rejection rate in descending order\n",
    "rejection_stats = rejection_stats.sort_values('Rejection Rate (%)', ascending=False)\n",
    "\n",
    "# Filter out models with zero rejection rate\n",
    "non_zero_rejection_stats = rejection_stats[rejection_stats['Rejection Rate (%)'] > 0]\n",
    "\n",
    "# Display rejection rate statistics for all models\n",
    "print(\"Model Rejection Rates Ranking (All Models):\")\n",
    "display(rejection_stats)\n",
    "\n",
    "# Display only non-zero rejection models\n",
    "print(\"\\nModel Rejection Rates Ranking (Non-Zero Only):\")\n",
    "display(non_zero_rejection_stats)\n",
    "\n",
    "\n",
    "# Set figure with higher resolution and better proportions\n",
    "plt.figure(figsize=(10, 6), dpi=100)\n",
    "\n",
    "# Create the plot with better-aligned x-tick positions, using only non-zero models\n",
    "x = np.arange(len(non_zero_rejection_stats.index))\n",
    "bars = plt.bar(x, non_zero_rejection_stats['Rejection Rate (%)'], color='skyblue', width=0.6)\n",
    "\n",
    "# Add value labels directly above each bar\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(x[i], height + 0.005, f'{height:.2f}%', \n",
    "             ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Set x-ticks at the centers of the bars\n",
    "plt.xticks(x, non_zero_rejection_stats.index, rotation=45, ha='right', fontsize=9)\n",
    "\n",
    "# Set chart title and axis labels\n",
    "plt.title('Rejection Rates by Model (Non-Zero Only)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Model', fontsize=12, labelpad=10)\n",
    "plt.ylabel('Rejection Rate (%)', fontsize=12)\n",
    "plt.yticks(fontsize=9)\n",
    "\n",
    "# Improve grid appearance\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3, zorder=0)\n",
    "\n",
    "# Add a thin border at the bottom\n",
    "plt.axhline(y=0, color='black', linewidth=0.8)\n",
    "\n",
    "# Ensure proper layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.2)  # Add more space at the bottom for x labels\n",
    "plt.savefig('figs/model_rejection_rates.pdf')\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liveideabench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
