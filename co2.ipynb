{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_path = \"./tokenizer/Qwen/Qwen2.5-72B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_tokens_from_local(text: str) -> int:\n",
    "    if not text:\n",
    "        return 0\n",
    "\n",
    "    return len(tokenizer.encode(text, add_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 使用示例\n",
    "\n",
    "text = \"Hello world! 你好世界！\"\n",
    "print(count_tokens_from_local(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_dict = {\n",
    "    'qwen/qwq-32b': {\n",
    "        'total_params': '32B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '32B'\n",
    "    },\n",
    "    'google/gemini-2.0-flash-exp:free': {\n",
    "        'total_params': '32B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '32B'\n",
    "    },\n",
    "    'google/gemini-2.0-flash-thinking-exp:free': {\n",
    "        'total_params': '32B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '32B'\n",
    "    },\n",
    "    'deepseek/deepseek-chat': {\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'deepseek/deepseek-r1-distill-llama-70b': {\n",
    "        'total_params': '70B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '70B'\n",
    "    },\n",
    "    'qwen/qwq-32b-preview': {\n",
    "        'total_params': '32B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '32B'\n",
    "    },\n",
    "    'anthropic/claude-3.7-sonnet:thinking': { # Estimated\n",
    "        'total_params': '175B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '175B'\n",
    "    },\n",
    "    'google/gemini-2.0-pro-exp-02-05:free': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'deepseek/deepseek-r1': {\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'anthropic/claude-3.5-haiku-20241022': { # Estimated\n",
    "        'total_params': '20B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '20B'\n",
    "    },\n",
    "    'deepseek/deepseek-r1-distill-qwen-32b': {\n",
    "        'total_params': '32B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '32B'\n",
    "    },\n",
    "    'openai/o1': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'openai/gpt-4.5-preview': { # Estimated\n",
    "        'total_params': '500B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '500B'\n",
    "    },\n",
    "    'mistralai/mistral-large-2411': {\n",
    "        'total_params': '123B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '123B'\n",
    "    },\n",
    "    'openai/o3-mini-high': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "     'amazon/nova-lite-v1': { # Estimated\n",
    "        'total_params': '8B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '8B'\n",
    "    },\n",
    "    'sammcj/qwen2.5-dracarys2-72b:Q4_K_M': {\n",
    "        'total_params': '72B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '72B'\n",
    "    },\n",
    "    'meta-llama/llama-3.1-405b-instruct': {\n",
    "        'total_params': '405B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '405B'\n",
    "    },\n",
    "    'openai/o3-mini': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'openai/gpt-4o-2024-11-20': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'openai/gpt-4o-mini': { # Estimated\n",
    "        'total_params': '7B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '7B'\n",
    "    },\n",
    "    'google/gemma-2-27b-it': {\n",
    "        'total_params': '27B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '27B'\n",
    "    },\n",
    "    'mistralai/mistral-small-24b-instruct-2501': {\n",
    "        'total_params': '24B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '24B'\n",
    "    },\n",
    "    'openai/gpt-4-turbo': { # Estimated\n",
    "        'total_params': '20B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '20B'\n",
    "    },\n",
    "    'mistralai/mistral-small': {\n",
    "        'total_params': '24B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '24B'\n",
    "    },\n",
    "    'openai/o1-mini': { # Estimated\n",
    "        'total_params': '7B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '7B'\n",
    "    },\n",
    "    'meta-llama/llama-3.1-70b-instruct': {\n",
    "        'total_params': '70B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '70B'\n",
    "    },\n",
    "    'qwen/qwen-2.5-72b-instruct': {\n",
    "        'total_params': '72B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '72B'\n",
    "    },\n",
    "    'x-ai/grok-2-1212': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'google/gemini-2.0-flash-lite-001': { # Estimated\n",
    "        'total_params': '8B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '8B'\n",
    "    },\n",
    "    'qwen/qwen-2.5-coder-32b-instruct': {\n",
    "        'total_params': '32B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '32B'\n",
    "    },\n",
    "    'qwen/qwen-2.5-7b-instruct': {\n",
    "        'total_params': '7B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '7B'\n",
    "    },\n",
    "    'qwen/qwen-max': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'anthropic/claude-3.5-sonnet': { # Estimated\n",
    "        'total_params': '175B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '175B'\n",
    "    },\n",
    "    'step-2-16k-202411': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'microsoft/phi-4': {\n",
    "        'total_params': '14B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '14B'\n",
    "    },\n",
    "    'google/gemini-pro-1.5': { # Estimated\n",
    "        'total_params': '671B',\n",
    "        'architecture': 'MoE',\n",
    "        'activated_params': '37B'\n",
    "    },\n",
    "    'meta-llama/llama-3.3-70b-instruct': {\n",
    "        'total_params': '70B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '70B'\n",
    "     },\n",
    "    'anthropic/claude-3.7-sonnet': { # Estimated\n",
    "        'total_params': '175B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '175B'\n",
    "    },\n",
    "    'anthropic/claude-3-opus': { # Estimated\n",
    "        'total_params': '500B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '500B'\n",
    "    },\n",
    "    'amazon/nova-pro-v1': { # Estimated\n",
    "        'total_params': '70B',\n",
    "        'architecture': 'Dense',\n",
    "        'activated_params': '70B'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# print(json.dumps(model_params_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecologits_calculator_per100_output_token(active_parameters, total_parameters):\n",
    "    # data from:\n",
    "    # https://huggingface.co/spaces/genai-impact/ecologits-calculator\n",
    "    res = {          # (Wh  , gCO2eq)\n",
    "        ('70B','70B'): (1.25, 0.762),\n",
    "        ('32B','32B'): (0.714, 0.437),\n",
    "        ('37B','671B'): (4.7, 2.88),\n",
    "        ('175B','175B'): (5.44, 3.32),\n",
    "        ('20B','20B'): (0.545, 0.335),\n",
    "        ('123B','123B'): (1.99, 1.22),\n",
    "        ('8B','8B'): (0.377, 0.232),\n",
    "        ('72B','72B'): (1.27, 0.779),\n",
    "        ('405B','405B'): (23.8, 14.5),\n",
    "        ('7B','7B'): (0.363, 0.223),\n",
    "        ('27B', '27B'): (0.643, 0.394),\n",
    "        ('24B', '24B'): (0.601, 0.369),\n",
    "        ('14B', '14B'): (0.461, 0.283),\n",
    "        ('500B', '500B'): (29.1, 17.8),\n",
    "        \n",
    "    }\n",
    "    return res[(active_parameters, total_parameters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen/qwq-32b                                       0.714 Wh                   0.437 gCO2eq              \n",
      "google/gemini-2.0-flash-exp:free                   0.714 Wh                   0.437 gCO2eq              \n",
      "google/gemini-2.0-flash-thinking-exp:free          0.714 Wh                   0.437 gCO2eq              \n",
      "deepseek/deepseek-chat                             4.7 Wh                   2.88 gCO2eq              \n",
      "deepseek/deepseek-r1-distill-llama-70b             1.25 Wh                   0.762 gCO2eq              \n",
      "qwen/qwq-32b-preview                               0.714 Wh                   0.437 gCO2eq              \n",
      "anthropic/claude-3.7-sonnet:thinking               5.44 Wh                   3.32 gCO2eq              \n",
      "google/gemini-2.0-pro-exp-02-05:free               4.7 Wh                   2.88 gCO2eq              \n",
      "deepseek/deepseek-r1                               4.7 Wh                   2.88 gCO2eq              \n",
      "anthropic/claude-3.5-haiku-20241022                0.545 Wh                   0.335 gCO2eq              \n",
      "deepseek/deepseek-r1-distill-qwen-32b              0.714 Wh                   0.437 gCO2eq              \n",
      "openai/o1                                          4.7 Wh                   2.88 gCO2eq              \n",
      "openai/gpt-4.5-preview                             29.1 Wh                   17.8 gCO2eq              \n",
      "mistralai/mistral-large-2411                       1.99 Wh                   1.22 gCO2eq              \n",
      "openai/o3-mini-high                                4.7 Wh                   2.88 gCO2eq              \n",
      "amazon/nova-lite-v1                                0.377 Wh                   0.232 gCO2eq              \n",
      "sammcj/qwen2.5-dracarys2-72b:Q4_K_M                1.27 Wh                   0.779 gCO2eq              \n",
      "meta-llama/llama-3.1-405b-instruct                 23.8 Wh                   14.5 gCO2eq              \n",
      "openai/o3-mini                                     4.7 Wh                   2.88 gCO2eq              \n",
      "openai/gpt-4o-2024-11-20                           4.7 Wh                   2.88 gCO2eq              \n",
      "openai/gpt-4o-mini                                 0.363 Wh                   0.223 gCO2eq              \n",
      "google/gemma-2-27b-it                              0.643 Wh                   0.394 gCO2eq              \n",
      "mistralai/mistral-small-24b-instruct-2501          0.601 Wh                   0.369 gCO2eq              \n",
      "openai/gpt-4-turbo                                 0.545 Wh                   0.335 gCO2eq              \n",
      "mistralai/mistral-small                            0.601 Wh                   0.369 gCO2eq              \n",
      "openai/o1-mini                                     0.363 Wh                   0.223 gCO2eq              \n",
      "meta-llama/llama-3.1-70b-instruct                  1.25 Wh                   0.762 gCO2eq              \n",
      "qwen/qwen-2.5-72b-instruct                         1.27 Wh                   0.779 gCO2eq              \n",
      "x-ai/grok-2-1212                                   4.7 Wh                   2.88 gCO2eq              \n",
      "google/gemini-2.0-flash-lite-001                   0.377 Wh                   0.232 gCO2eq              \n",
      "qwen/qwen-2.5-coder-32b-instruct                   0.714 Wh                   0.437 gCO2eq              \n",
      "qwen/qwen-2.5-7b-instruct                          0.363 Wh                   0.223 gCO2eq              \n",
      "qwen/qwen-max                                      4.7 Wh                   2.88 gCO2eq              \n",
      "anthropic/claude-3.5-sonnet                        5.44 Wh                   3.32 gCO2eq              \n",
      "step-2-16k-202411                                  4.7 Wh                   2.88 gCO2eq              \n",
      "microsoft/phi-4                                    0.461 Wh                   0.283 gCO2eq              \n",
      "google/gemini-pro-1.5                              4.7 Wh                   2.88 gCO2eq              \n",
      "meta-llama/llama-3.3-70b-instruct                  1.25 Wh                   0.762 gCO2eq              \n",
      "anthropic/claude-3.7-sonnet                        5.44 Wh                   3.32 gCO2eq              \n",
      "anthropic/claude-3-opus                            29.1 Wh                   17.8 gCO2eq              \n",
      "amazon/nova-pro-v1                                 1.25 Wh                   0.762 gCO2eq              \n"
     ]
    }
   ],
   "source": [
    "for k,v in model_params_dict.items():\n",
    "    # print(k)\n",
    "    active = v['activated_params']\n",
    "    \n",
    "    total = v['total_params']\n",
    "    res = ecologits_calculator_per100_output_token(active, total)\n",
    "    # print(res)\n",
    "    \n",
    "    energy_Wh = res[0]\n",
    "    GHG_emissions = res[1]\n",
    "    \n",
    "    print(k.ljust(50),energy_Wh, 'Wh'.ljust(20),GHG_emissions, 'gCO2eq'.ljust(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./csvs/liveideabench_hf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                       NaN\n",
       "1         Let me evaluate this AI-CAD plug-in idea for r...\n",
       "2                                                       NaN\n",
       "3                                                       NaN\n",
       "4                                                       NaN\n",
       "                                ...                        \n",
       "286487    Let's break down this idea and evaluate it acr...\n",
       "286488                                                  NaN\n",
       "286489                                                  NaN\n",
       "286490                                                  NaN\n",
       "286491                                                  NaN\n",
       "Name: critique_reasoning, Length: 286492, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['critique_reasoning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         {'originality': 8, 'feasibility': 6, 'clarity'...\n",
       "1         {'originality': 7, 'feasibility': 6, 'clarity'...\n",
       "2         {'originality': 9, 'feasibility': 6, 'clarity'...\n",
       "3         {'originality': 7, 'feasibility': 6, 'clarity'...\n",
       "4         {'originality': 8, 'feasibility': 6, 'clarity'...\n",
       "                                ...                        \n",
       "286487    {'originality': 4, 'feasibility': 6, 'clarity'...\n",
       "286488    {'originality': 5, 'feasibility': 6, 'clarity'...\n",
       "286489    {'originality': 8, 'feasibility': 7, 'clarity'...\n",
       "286490    {'originality': 7, 'feasibility': 5, 'clarity'...\n",
       "286491    {'originality': 6, 'feasibility': 5, 'clarity'...\n",
       "Name: parsed_scores, Length: 286492, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['parsed_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['keywords', 'idea_model', 'critic_model', 'idea', 'raw_critique',\n",
       "       'parsed_scores', 'critique_reasoning', 'full_response',\n",
       "       'first_was_rejected', 'first_reject_response', 'idea_length_in_char',\n",
       "       'idea_length_in_words', 'scores', 'originality', 'feasibility',\n",
       "       'clarity', 'fluency', 'avg', 'full_response_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95738"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['idea'].unique().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286492"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CO2 emission of generating ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理模型:  10%|▉         | 4/41 [00:16<01:44,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idea error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理模型: 100%|██████████| 41/41 [01:45<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qwen/qwq-32b': 3272395, 'google/gemini-2.0-flash-exp:free': 289855, 'google/gemini-2.0-flash-thinking-exp:free': 268822, 'deepseek/deepseek-chat': 339689, 'deepseek/deepseek-r1-distill-llama-70b': 1054313, 'qwen/qwq-32b-preview': 3774662, 'anthropic/claude-3.7-sonnet:thinking': 4993782, 'google/gemini-2.0-pro-exp-02-05:free': 315840, 'deepseek/deepseek-r1': 1642147, 'anthropic/claude-3.5-haiku-20241022': 278554, 'deepseek/deepseek-r1-distill-qwen-32b': 1429143, 'openai/o1': 240612, 'openai/gpt-4.5-preview': 287003, 'mistralai/mistral-large-2411': 292152, 'openai/o3-mini-high': 232967, 'amazon/nova-lite-v1': 222522, 'sammcj/qwen2.5-dracarys2-72b:Q4_K_M': 261955, 'meta-llama/llama-3.1-405b-instruct': 317736, 'openai/o3-mini': 223555, 'openai/gpt-4o-2024-11-20': 322091, 'openai/gpt-4o-mini': 268069, 'google/gemma-2-27b-it': 215656, 'mistralai/mistral-small-24b-instruct-2501': 349875, 'openai/gpt-4-turbo': 300866, 'mistralai/mistral-small': 295047, 'openai/o1-mini': 292354, 'meta-llama/llama-3.1-70b-instruct': 331842, 'qwen/qwen-2.5-72b-instruct': 284860, 'x-ai/grok-2-1212': 259922, 'google/gemini-2.0-flash-lite-001': 294225, 'qwen/qwen-2.5-coder-32b-instruct': 260045, 'qwen/qwen-2.5-7b-instruct': 226013, 'qwen/qwen-max': 167514, 'anthropic/claude-3.5-sonnet': 223678, 'step-2-16k-202411': 317633, 'microsoft/phi-4': 396819, 'google/gemini-pro-1.5': 207097, 'meta-llama/llama-3.3-70b-instruct': 131481, 'anthropic/claude-3.7-sonnet': 304098, 'anthropic/claude-3-opus': 287743, 'amazon/nova-pro-v1': 198872}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def count_tokens_by_idea_model(df):\n",
    "    # 创建一个字典来存储结果\n",
    "    token_counts = {}\n",
    "    \n",
    "    # 获取所有唯一的idea_model\n",
    "    unique_models = df['idea_model'].unique()\n",
    "    \n",
    "    # 使用tqdm显示模型处理进度\n",
    "    for model in tqdm(unique_models, desc=\"处理模型\"):\n",
    "        # 获取该模型生成的所有idea\n",
    "        model_ideas = df[df['idea_model'] == model]['full_response']\n",
    "        \n",
    "        # 使用set获取唯一的idea\n",
    "        unique_ideas = set(model_ideas)\n",
    "        \n",
    "        # 计算所有唯一idea的token总数，使用tqdm显示每个模型内的idea处理进度\n",
    "        total_tokens = 0\n",
    "        for idea in tqdm(unique_ideas, desc=f\"处理{model}的idea\", leave=False):\n",
    "            try:\n",
    "                total_tokens += count_tokens_from_local(idea)\n",
    "            except:\n",
    "                print('idea error:',idea)\n",
    "        \n",
    "        # 存储结果\n",
    "        token_counts[model] = total_tokens\n",
    "    \n",
    "    return token_counts\n",
    "\n",
    "# 使用函数计算各个idea_model的token总数\n",
    "result = count_tokens_by_idea_model(df)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'amazon/nova-lite-v1': 222522,\n",
      " 'amazon/nova-pro-v1': 198872,\n",
      " 'anthropic/claude-3-opus': 287743,\n",
      " 'anthropic/claude-3.5-haiku-20241022': 278554,\n",
      " 'anthropic/claude-3.5-sonnet': 223678,\n",
      " 'anthropic/claude-3.7-sonnet': 304098,\n",
      " 'anthropic/claude-3.7-sonnet:thinking': 4993782,\n",
      " 'deepseek/deepseek-chat': 339689,\n",
      " 'deepseek/deepseek-r1': 1642147,\n",
      " 'deepseek/deepseek-r1-distill-llama-70b': 1054313,\n",
      " 'deepseek/deepseek-r1-distill-qwen-32b': 1429143,\n",
      " 'google/gemini-2.0-flash-exp:free': 289855,\n",
      " 'google/gemini-2.0-flash-lite-001': 294225,\n",
      " 'google/gemini-2.0-flash-thinking-exp:free': 268822,\n",
      " 'google/gemini-2.0-pro-exp-02-05:free': 315840,\n",
      " 'google/gemini-pro-1.5': 207097,\n",
      " 'google/gemma-2-27b-it': 215656,\n",
      " 'meta-llama/llama-3.1-405b-instruct': 317736,\n",
      " 'meta-llama/llama-3.1-70b-instruct': 331842,\n",
      " 'meta-llama/llama-3.3-70b-instruct': 131481,\n",
      " 'microsoft/phi-4': 396819,\n",
      " 'mistralai/mistral-large-2411': 292152,\n",
      " 'mistralai/mistral-small': 295047,\n",
      " 'mistralai/mistral-small-24b-instruct-2501': 349875,\n",
      " 'openai/gpt-4-turbo': 300866,\n",
      " 'openai/gpt-4.5-preview': 287003,\n",
      " 'openai/gpt-4o-2024-11-20': 322091,\n",
      " 'openai/gpt-4o-mini': 268069,\n",
      " 'openai/o1': 240612,\n",
      " 'openai/o1-mini': 292354,\n",
      " 'openai/o3-mini': 223555,\n",
      " 'openai/o3-mini-high': 232967,\n",
      " 'qwen/qwen-2.5-72b-instruct': 284860,\n",
      " 'qwen/qwen-2.5-7b-instruct': 226013,\n",
      " 'qwen/qwen-2.5-coder-32b-instruct': 260045,\n",
      " 'qwen/qwen-max': 167514,\n",
      " 'qwen/qwq-32b': 3272395,\n",
      " 'qwen/qwq-32b-preview': 3774662,\n",
      " 'sammcj/qwen2.5-dracarys2-72b:Q4_K_M': 261955,\n",
      " 'step-2-16k-202411': 317633,\n",
      " 'x-ai/grok-2-1212': 259922}\n"
     ]
    }
   ],
   "source": [
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CO2 emission of critic LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理critic模型:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理critic模型: 100%|██████████| 10/10 [06:24<00:00, 38.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'openai/o3-mini-high': 2799760, 'anthropic/claude-3.7-sonnet:thinking': 25938526, 'openai/gpt-4.5-preview': 3611186, 'qwen/qwen-max': 3472270, 'google/gemini-2.0-flash-thinking-exp:free': 3220243, 'anthropic/claude-3.5-sonnet': 4158124, 'google/gemini-2.0-pro-exp-02-05:free': 3576721, 'deepseek/deepseek-chat': 3925279, 'deepseek/deepseek-r1': 14263413, 'qwen/qwq-32b': 19476097}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def count_tokens_by_critic_model(df):\n",
    "    # 创建一个字典来存储结果\n",
    "    token_counts = {}\n",
    "    \n",
    "    # 获取所有唯一的critic_model\n",
    "    unique_critic_models = df['critic_model'].unique()\n",
    "    \n",
    "    # 使用tqdm显示critic模型处理进度\n",
    "    for critic_model in tqdm(unique_critic_models, desc=\"处理critic模型\"):\n",
    "        # 筛选该critic模型的所有行\n",
    "        critic_rows = df[df['critic_model'] == critic_model]\n",
    "        \n",
    "        # 计算该critic模型使用的总token数\n",
    "        total_tokens = 0\n",
    "        \n",
    "        # 遍历该critic模型的每一行\n",
    "        for _, row in tqdm(critic_rows.iterrows(), desc=f\"处理{critic_model}的critique\", leave=False, total=len(critic_rows)):\n",
    "            try:\n",
    "                # 处理raw_critique\n",
    "                if not pd.isna(row['raw_critique']):\n",
    "                    total_tokens += count_tokens_from_local(row['raw_critique'])\n",
    "                \n",
    "                # 处理critique_reasoning (可能为NaN)\n",
    "                if not pd.isna(row['critique_reasoning']):\n",
    "                    total_tokens += count_tokens_from_local(row['critique_reasoning'])\n",
    "            except Exception as e:\n",
    "                print(f'Error processing row: {e}')\n",
    "                print(f'raw_critique: {row[\"raw_critique\"][:100]}...')\n",
    "                print(f'critique_reasoning: {row[\"critique_reasoning\"][:100] if not pd.isna(row[\"critique_reasoning\"]) else \"NaN\"}...')\n",
    "        \n",
    "        # 存储结果\n",
    "        token_counts[critic_model] = total_tokens\n",
    "    \n",
    "    return token_counts\n",
    "\n",
    "# 使用函数计算各个critic_model的token总数\n",
    "critic_token_results = count_tokens_by_critic_model(df)\n",
    "print(critic_token_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anthropic/claude-3.5-sonnet': 4158124,\n",
      " 'anthropic/claude-3.7-sonnet:thinking': 25938526,\n",
      " 'deepseek/deepseek-chat': 3925279,\n",
      " 'deepseek/deepseek-r1': 14263413,\n",
      " 'google/gemini-2.0-flash-thinking-exp:free': 3220243,\n",
      " 'google/gemini-2.0-pro-exp-02-05:free': 3576721,\n",
      " 'openai/gpt-4.5-preview': 3611186,\n",
      " 'openai/o3-mini-high': 2799760,\n",
      " 'qwen/qwen-max': 3472270,\n",
      " 'qwen/qwq-32b': 19476097}\n"
     ]
    }
   ],
   "source": [
    "pprint(critic_token_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# i# 假设我们添加一个是否开源的信息字典\n",
    "open_source_models = {\n",
    "    'gemini-pro-1.5': False,\n",
    "    'qwq-32b-preview': True,\n",
    "    'qwq-32b': True,\n",
    "    'o1-preview': False,\n",
    "    'claude-3.5-sonnet': False,\n",
    "    'claude-3.7-sonnet': False,\n",
    "    'gemini-2.0-flash-exp': False,\n",
    "    'qwen2.5-dracarys2-72b': True,\n",
    "    'nova-pro-v1': False,\n",
    "    'gpt-4o-2024-11-20': False,\n",
    "    'mistral-large-2411': True,\n",
    "    'llama-3.1-nemotron-70b-instruct': True,\n",
    "    'qwen-2.5-coder-32b-instruct': True,\n",
    "    'llama-3.1-405b-instruct': True,\n",
    "    'o1-mini': False,\n",
    "    'qwen-2.5-72b-instruct': True,\n",
    "    'claude-3.5-haiku': False,\n",
    "    'step-2-16k': False,\n",
    "    'grok-2-1212': False,\n",
    "    'gpt-4o-mini': False,\n",
    "    'deepseek-chat': True,\n",
    "\n",
    "    'deepseek-r1': True,\n",
    "    'deepseek-r1-distill-llama-70b': True,\n",
    "\n",
    "    'minimax-01': True,\n",
    "    'mistral-nemo': True,\n",
    "    'phi-4': True,\n",
    "    'claude-3-opus': False,\n",
    "\n",
    "    'llama-3.3-70b-instruct': True,\n",
    "\n",
    "    'QwQ-32B-Preview-IdeaWhiz-v1': True,\n",
    "    'gemini-2.0-flash-001': False,\n",
    "    \n",
    "    'mistral-small':True,\n",
    "    \n",
    "    'gemini-2.0-flash-thinking-exp':False,\n",
    "    \n",
    "    'deepseek-r1-distill-qwen-32b':True,\n",
    "    'gemini-2.0-flash-lite-001':False,\n",
    "    'codestral-2501':True,\n",
    "    \n",
    "    'qwen-max':False,\n",
    "    'qwen-plus':False,\n",
    "    'o3-mini':False,\n",
    "    \n",
    "    'o3-mini-high':False,\n",
    "    \"gemma-2-27b-it\":True,\n",
    "    \"mistral-small-24b-instruct-2501\":True,\n",
    "\n",
    "    # small models\n",
    "    'mistral-small': True,\n",
    "    'qwen-2.5-7b-instruct':True,\n",
    "    'phi-3-mini-128k-instruct':True,\n",
    "    'llama-3.1-8b-instruct':True,\n",
    "    'nova-micro-v1':False,\n",
    "    'gemma-2-9b-it':True,\n",
    "    'command-r-08-2024':True,\n",
    "    'gemini-2.0-pro-exp-02-05':False,\n",
    "    'o1':False,\n",
    "    'llama-3.1-70b-instruct':True,\n",
    "    'step-2-16k-202411':False,\n",
    "    'gpt-4.5-preview':False,\n",
    "    'claude-3.5-haiku-20241022':False,\n",
    "    'gpt-4-turbo':False,\n",
    "    'nova-lite-v1':False,\n",
    "    'google/gemini-2.0-flash-exp:free':False,\n",
    "    'anthropic/claude-3.7-sonnet':False,\n",
    "    'openai/o1':False,\n",
    "    'openai/o3-mini':False,\n",
    "    'openai/o1-mini':False,\n",
    "    'openai/gpt-4o-2024-11-20':False,\n",
    "    'deepseek/deepseek-r1-distill-llama-70b':True,\n",
    "    'google/gemini-pro-1.5':False,\n",
    "    'x-ai/grok-2-1212':False,\n",
    "    'google/gemini-2.0-flash-lite-001':False,\n",
    "    'sammcj/qwen2.5-dracarys2-72b:Q4_K_M':True,\n",
    "    'meta-llama/llama-3.1-405b-instruct':True,\n",
    "    'qwen/qwen-2.5-72b-instruct':True,\n",
    "    'openai/gpt-4-turbo':False,\n",
    "    'meta-llama/llama-3.3-70b-instruct':True,\n",
    "    'anthropic/claude-3-opus':False,\n",
    "    'mistralai/mistral-large-2411':True,\n",
    "    'qwen/qwen-2.5-coder-32b-instruct':True,\n",
    "    'deepseek/deepseek-r1-distill-qwen-32b':True,\n",
    "    'meta-llama/llama-3.1-70b-instruct':True,\n",
    "    'amazon/nova-pro-v1':False,\n",
    "    'anthropic/claude-3.5-haiku-20241022':False,\n",
    "    'mistralai/mistral-small-24b-instruct-2501':True,\n",
    "    'microsoft/phi-4':True,\n",
    "    'openai/gpt-4o-mini':False,\n",
    "    'qwen/qwq-32b-preview':True,\n",
    "    'amazon/nova-lite-v1':False,\n",
    "    'qwen/qwen-2.5-7b-instruct':True,\n",
    "    'mistralai/mistral-small':True,\n",
    "    'google/gemma-2-27b-it':True,\n",
    "    'anthropic/claude-3.7-sonnet:thinking':False,\n",
    "    'openai/o3-mini-high':False,\n",
    "    'openai/gpt-4.5-preview':False,\n",
    "    'qwen/qwq-32b':True,\n",
    "    'deepseek/deepseek-r1':True,\n",
    "    'google/gemini-2.0-flash-thinking-exp:free':False,\n",
    "    'google/gemini-2.0-pro-exp-02-05:free':False,\n",
    "    'qwen/qwen-max':False,\n",
    "    'deepseek/deepseek-chat':True,\n",
    "    'anthropic/claude-3.5-sonnet':False,\n",
    "    'claude-3.7-sonnet:thinking':False,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Idea Model Environmental Impact: 100%|██████████| 41/41 [00:00<00:00, 105500.90it/s]\n",
      "Calculating Critic Model Environmental Impact: 100%|██████████| 10/10 [00:00<00:00, 88674.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[htbp]\n",
      "\\centering\n",
      "\\caption{Energy Consumption and Carbon Emissions by Model: As Idea Generator and Critic}\n",
      "\\resizebox{0.8\\textwidth}{!}{%\n",
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      "\\multirow{2}{*}{Model Name} & \\multicolumn{2}{c}{As Idea Model} & \\multicolumn{2}{c}{As Critic Model} & \\multicolumn{2}{c}{Total} \\\\\n",
      "\\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n",
      "& \\makecell{Energy\\\\(kWh)} & \\makecell{Emissions\\\\(kgCO$_2$eq.)} & \\makecell{Energy\\\\(kWh)} & \\makecell{Emissions\\\\(kgCO$_2$eq.)} & \\makecell{Energy\\\\(kWh)} & \\makecell{Emissions\\\\(kgCO$_2$eq.)} \\\\\n",
      "\\midrule\n",
      "\\texttt{claude-3-opus$^*$} & 83.73 & 51.22 & - & - & 83.73 & 51.22 \\\\\n",
      "\\texttt{claude-3.5-haiku-20241022$^*$} & 1.52 & 0.93 & - & - & 1.52 & 0.93 \\\\\n",
      "\\texttt{claude-3.5-sonnet$^*$} & 12.17 & 7.43 & 226.20 & 138.05 & 238.37 & 145.48 \\\\\n",
      "\\texttt{claude-3.7-sonnet$^*$} & 16.54 & 10.10 & - & - & 16.54 & 10.10 \\\\\n",
      "\\texttt{claude-3.7-sonnet:thinking$^*$} & 271.66 & 165.79 & 1411.06 & 861.16 & 1682.72 & 1026.95 \\\\\n",
      "\\texttt{deepseek-chat} & 15.97 & 9.78 & 184.49 & 113.05 & 200.45 & 122.83 \\\\\n",
      "\\texttt{deepseek-r1} & 77.18 & 47.29 & 670.38 & 410.79 & 747.56 & 458.08 \\\\\n",
      "\\texttt{deepseek-r1-distill-llama-70b} & 13.18 & 8.03 & - & - & 13.18 & 8.03 \\\\\n",
      "\\texttt{deepseek-r1-distill-qwen-32b} & 10.20 & 6.25 & - & - & 10.20 & 6.25 \\\\\n",
      "\\texttt{gemini-2.0-flash-exp$^*$} & 2.07 & 1.27 & - & - & 2.07 & 1.27 \\\\\n",
      "\\texttt{gemini-2.0-flash-lite-001$^*$} & 1.11 & 0.68 & - & - & 1.11 & 0.68 \\\\\n",
      "\\texttt{gemini-2.0-flash-thinking-exp$^*$} & 1.92 & 1.17 & 22.99 & 14.07 & 24.91 & 15.25 \\\\\n",
      "\\texttt{gemini-2.0-pro-exp-02-05$^*$} & 14.84 & 9.10 & 168.11 & 103.01 & 182.95 & 112.11 \\\\\n",
      "\\texttt{gemini-pro-1.5$^*$} & 9.73 & 5.96 & - & - & 9.73 & 5.96 \\\\\n",
      "\\texttt{gemma-2-27b-it} & 1.39 & 0.85 & - & - & 1.39 & 0.85 \\\\\n",
      "\\texttt{gpt-4-turbo$^*$} & 1.64 & 1.01 & - & - & 1.64 & 1.01 \\\\\n",
      "\\texttt{gpt-4.5-preview$^*$} & 83.52 & 51.09 & 1050.86 & 642.79 & 1134.37 & 693.88 \\\\\n",
      "\\texttt{gpt-4o-2024-11-20$^*$} & 15.14 & 9.28 & - & - & 15.14 & 9.28 \\\\\n",
      "\\texttt{gpt-4o-mini$^*$} & 0.97 & 0.60 & - & - & 0.97 & 0.60 \\\\\n",
      "\\texttt{grok-2-1212$^*$} & 12.22 & 7.49 & - & - & 12.22 & 7.49 \\\\\n",
      "\\texttt{llama-3.1-405b-instruct} & 75.62 & 46.07 & - & - & 75.62 & 46.07 \\\\\n",
      "\\texttt{llama-3.1-70b-instruct} & 4.15 & 2.53 & - & - & 4.15 & 2.53 \\\\\n",
      "\\texttt{llama-3.3-70b-instruct} & 1.64 & 1.00 & - & - & 1.64 & 1.00 \\\\\n",
      "\\texttt{mistral-large-2411} & 5.81 & 3.56 & - & - & 5.81 & 3.56 \\\\\n",
      "\\texttt{mistral-small} & 1.77 & 1.09 & - & - & 1.77 & 1.09 \\\\\n",
      "\\texttt{mistral-small-24b-instruct-2501} & 2.10 & 1.29 & - & - & 2.10 & 1.29 \\\\\n",
      "\\texttt{nova-lite-v1$^*$} & 0.84 & 0.52 & - & - & 0.84 & 0.52 \\\\\n",
      "\\texttt{nova-pro-v1$^*$} & 2.49 & 1.52 & - & - & 2.49 & 1.52 \\\\\n",
      "\\texttt{o1$^*$} & 11.31 & 6.93 & - & - & 11.31 & 6.93 \\\\\n",
      "\\texttt{o1-mini$^*$} & 1.06 & 0.65 & - & - & 1.06 & 0.65 \\\\\n",
      "\\texttt{o3-mini$^*$} & 10.51 & 6.44 & - & - & 10.51 & 6.44 \\\\\n",
      "\\texttt{o3-mini-high$^*$} & 10.95 & 6.71 & 131.59 & 80.63 & 142.54 & 87.34 \\\\\n",
      "\\texttt{phi-4} & 1.83 & 1.12 & - & - & 1.83 & 1.12 \\\\\n",
      "\\texttt{qwen-2.5-72b-instruct} & 3.62 & 2.22 & - & - & 3.62 & 2.22 \\\\\n",
      "\\texttt{qwen-2.5-7b-instruct} & 0.82 & 0.50 & - & - & 0.82 & 0.50 \\\\\n",
      "\\texttt{qwen-2.5-coder-32b-instruct} & 1.86 & 1.14 & - & - & 1.86 & 1.14 \\\\\n",
      "\\texttt{qwen-max$^*$} & 7.87 & 4.82 & 163.20 & 100.00 & 171.07 & 104.83 \\\\\n",
      "\\texttt{qwen2.5-dracarys2-72b} & 3.33 & 2.04 & - & - & 3.33 & 2.04 \\\\\n",
      "\\texttt{qwq-32b} & 23.36 & 14.30 & 139.06 & 85.11 & 162.42 & 99.41 \\\\\n",
      "\\texttt{qwq-32b-preview} & 26.95 & 16.50 & - & - & 26.95 & 16.50 \\\\\n",
      "\\texttt{step-2-16k-202411$^*$} & 14.93 & 9.15 & - & - & 14.93 & 9.15 \\\\\n",
      "\\midrule\n",
      "Total & 859.52 & 525.41 & 4167.92 & 2548.66 & 5027.45 & 3074.07 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\label{tab:combined_emissions}\n",
      "\\caption*{\\footnotesize{$^*$Non-open models: environmental impact values are rough estimates and may not be accurate.}}\n",
      "\\end{table}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def clean_model_name(name):\n",
    "    # Handle paths by removing the part before \"/\"\n",
    "    if '/' in name:\n",
    "        name = name.split('/', 1)[1]  # Split only once, take the second part\n",
    "    \n",
    "    # Handle specific suffixes\n",
    "    if ':' in name:\n",
    "        base_name, suffix = name.split(':', 1)\n",
    "        # Only remove if the suffix is of a specific type\n",
    "        if suffix in ['Q4_K_M', 'free']:\n",
    "            return base_name\n",
    "        # Otherwise keep the full name, including colon and suffix\n",
    "        return name\n",
    "    \n",
    "    return name\n",
    "\n",
    "def calculate_combined_emissions_table(idea_token_counts, critic_token_counts, model_params_dict, open_source_models):\n",
    "    # Create dictionaries to store results\n",
    "    idea_emissions = {}\n",
    "    critic_emissions = {}\n",
    "    all_models = set(list(idea_token_counts.keys()) + list(critic_token_counts.keys()))\n",
    "    \n",
    "    # Calculate energy and emissions for idea models\n",
    "    for model_name, token_count in tqdm(idea_token_counts.items(), desc=\"Calculating Idea Model Environmental Impact\"):\n",
    "        if model_name in model_params_dict:\n",
    "            # Get model parameters\n",
    "            active = model_params_dict[model_name]['activated_params']\n",
    "            total = model_params_dict[model_name]['total_params']\n",
    "            \n",
    "            # Calculate energy and emissions per 100 output tokens\n",
    "            res = ecologits_calculator_per100_output_token(active, total)\n",
    "            energy_Wh_per100 = res[0]\n",
    "            GHG_emissions_per100 = res[1]\n",
    "            \n",
    "            # Calculate total energy and emissions (convert Wh to kWh and gCO2eq to kgCO2eq)\n",
    "            total_energy_kWh = ((token_count / 100) * energy_Wh_per100) / 1000\n",
    "            total_GHG_emissions_kg = ((token_count / 100) * GHG_emissions_per100) / 1000\n",
    "            \n",
    "            # Clean the model name\n",
    "            clean_name = clean_model_name(model_name)\n",
    "            \n",
    "            # Save results\n",
    "            idea_emissions[model_name] = {\n",
    "                'clean_name': clean_name,\n",
    "                'token_count': token_count,\n",
    "                'energy_kWh': total_energy_kWh,\n",
    "                'GHG_emissions_kg': total_GHG_emissions_kg\n",
    "            }\n",
    "        else:\n",
    "            print(f\"% Warning: Parameters for model '{model_name}' not found\")\n",
    "    \n",
    "    # Calculate energy and emissions for critic models\n",
    "    for model_name, token_count in tqdm(critic_token_counts.items(), desc=\"Calculating Critic Model Environmental Impact\"):\n",
    "        if model_name in model_params_dict:\n",
    "            # Get model parameters\n",
    "            active = model_params_dict[model_name]['activated_params']\n",
    "            total = model_params_dict[model_name]['total_params']\n",
    "            \n",
    "            # Calculate energy and emissions per 100 output tokens\n",
    "            res = ecologits_calculator_per100_output_token(active, total)\n",
    "            energy_Wh_per100 = res[0]\n",
    "            GHG_emissions_per100 = res[1]\n",
    "            \n",
    "            # Calculate total energy and emissions (convert Wh to kWh and gCO2eq to kgCO2eq)\n",
    "            total_energy_kWh = ((token_count / 100) * energy_Wh_per100) / 1000\n",
    "            total_GHG_emissions_kg = ((token_count / 100) * GHG_emissions_per100) / 1000\n",
    "            \n",
    "            # Clean the model name\n",
    "            clean_name = clean_model_name(model_name)\n",
    "            \n",
    "            # Save results\n",
    "            critic_emissions[model_name] = {\n",
    "                'clean_name': clean_name,\n",
    "                'token_count': token_count,\n",
    "                'energy_kWh': total_energy_kWh,\n",
    "                'GHG_emissions_kg': total_GHG_emissions_kg\n",
    "            }\n",
    "        else:\n",
    "            print(f\"% Warning: Parameters for model '{model_name}' not found\")\n",
    "    \n",
    "    # Start LaTeX table\n",
    "    print(\"\\\\begin{table}[htbp]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\caption{Energy Consumption and Carbon Emissions by Model: As Idea Generator and Critic}\")\n",
    "    # Add resizebox command to scale the table to 0.8\\textwidth\n",
    "    print(\"\\\\resizebox{0.8\\\\textwidth}{!}{%\")\n",
    "    # Make sure to include makecell package in the preamble: \\usepackage{makecell}\n",
    "    # Make sure to include graphicx package in the preamble: \\usepackage{graphicx}\n",
    "    print(\"\\\\begin{tabular}{lrrrrrr}\")\n",
    "    print(\"\\\\toprule\")\n",
    "    print(\"\\\\multirow{2}{*}{Model Name} & \\\\multicolumn{2}{c}{As Idea Model} & \\\\multicolumn{2}{c}{As Critic Model} & \\\\multicolumn{2}{c}{Total} \\\\\\\\\")\n",
    "    print(\"\\\\cmidrule(lr){2-3} \\\\cmidrule(lr){4-5} \\\\cmidrule(lr){6-7}\")\n",
    "    print(\"& \\\\makecell{Energy\\\\\\\\(kWh)} & \\\\makecell{Emissions\\\\\\\\(kgCO$_2$eq.)} & \\\\makecell{Energy\\\\\\\\(kWh)} & \\\\makecell{Emissions\\\\\\\\(kgCO$_2$eq.)} & \\\\makecell{Energy\\\\\\\\(kWh)} & \\\\makecell{Emissions\\\\\\\\(kgCO$_2$eq.)} \\\\\\\\\")\n",
    "    print(\"\\\\midrule\")\n",
    "    \n",
    "    # Create a dictionary containing all models for combined results\n",
    "    combined_results = {}\n",
    "    \n",
    "    for model in all_models:\n",
    "        if model in model_params_dict:\n",
    "            clean_name = clean_model_name(model) if model in idea_emissions or model in critic_emissions else \"Unknown Model\"\n",
    "            \n",
    "            # Check if the model is open source\n",
    "            is_open_source = open_source_models.get(model, True)  # Default to True if not in the dictionary\n",
    "            \n",
    "            # Get Idea model data (if available)\n",
    "            idea_energy = idea_emissions[model]['energy_kWh'] if model in idea_emissions else 0\n",
    "            idea_emissions_val = idea_emissions[model]['GHG_emissions_kg'] if model in idea_emissions else 0\n",
    "            \n",
    "            # Get Critic model data (if available)\n",
    "            critic_energy = critic_emissions[model]['energy_kWh'] if model in critic_emissions else 0\n",
    "            critic_emissions_val = critic_emissions[model]['GHG_emissions_kg'] if model in critic_emissions else 0\n",
    "            \n",
    "            # Calculate totals\n",
    "            total_energy = idea_energy + critic_energy\n",
    "            total_emissions_val = idea_emissions_val + critic_emissions_val\n",
    "            \n",
    "            # Store results for sorting\n",
    "            combined_results[model] = {\n",
    "                'clean_name': clean_name,\n",
    "                'is_open_source': is_open_source,\n",
    "                'idea_energy': idea_energy,\n",
    "                'idea_emissions': idea_emissions_val,\n",
    "                'critic_energy': critic_energy,\n",
    "                'critic_emissions': critic_emissions_val,\n",
    "                'total_energy': total_energy,\n",
    "                'total_emissions': total_emissions_val\n",
    "            }\n",
    "    \n",
    "    # Create a list of tuples with (clean_name, original_name) for sorting\n",
    "    model_name_pairs = [(data['clean_name'], original_name) for original_name, data in combined_results.items()]\n",
    "    \n",
    "    # Sort by cleaned name and print results\n",
    "    for clean_name, original_name in sorted(model_name_pairs, key=lambda x: x[0].lower()):\n",
    "        data = combined_results[original_name]\n",
    "        \n",
    "        # Format values: replace 0.00 with \"-\"\n",
    "        idea_energy_str = f\"{data['idea_energy']:.2f}\" if data['idea_energy'] > 0 else \"-\"\n",
    "        idea_emissions_str = f\"{data['idea_emissions']:.2f}\" if data['idea_emissions'] > 0 else \"-\"\n",
    "        critic_energy_str = f\"{data['critic_energy']:.2f}\" if data['critic_energy'] > 0 else \"-\"\n",
    "        critic_emissions_str = f\"{data['critic_emissions']:.2f}\" if data['critic_emissions'] > 0 else \"-\"\n",
    "        total_energy_str = f\"{data['total_energy']:.2f}\" if data['total_energy'] > 0 else \"-\"\n",
    "        total_emissions_str = f\"{data['total_emissions']:.2f}\" if data['total_emissions'] > 0 else \"-\"\n",
    "        \n",
    "        # Add asterisk to non-open source models\n",
    "        display_name = clean_name\n",
    "        if not data['is_open_source']:\n",
    "            display_name = f\"{clean_name}$^*$\"\n",
    "        \n",
    "        # Print results in LaTeX format\n",
    "        print(f\"\\\\texttt{{{display_name}}} & {idea_energy_str} & {idea_emissions_str} & {critic_energy_str} & {critic_emissions_str} & {total_energy_str} & {total_emissions_str} \\\\\\\\\")\n",
    "    \n",
    "    # Calculate total energy and emissions\n",
    "    total_idea_energy = sum(data['idea_energy'] for data in combined_results.values())\n",
    "    total_idea_emissions = sum(data['idea_emissions'] for data in combined_results.values())\n",
    "    total_critic_energy = sum(data['critic_energy'] for data in combined_results.values())\n",
    "    total_critic_emissions = sum(data['critic_emissions'] for data in combined_results.values())\n",
    "    grand_total_energy = total_idea_energy + total_critic_energy\n",
    "    grand_total_emissions = total_idea_emissions + total_critic_emissions\n",
    "    \n",
    "    # Format total values\n",
    "    total_idea_energy_str = f\"{total_idea_energy:.2f}\" if total_idea_energy > 0 else \"-\"\n",
    "    total_idea_emissions_str = f\"{total_idea_emissions:.2f}\" if total_idea_emissions > 0 else \"-\"\n",
    "    total_critic_energy_str = f\"{total_critic_energy:.2f}\" if total_critic_energy > 0 else \"-\"\n",
    "    total_critic_emissions_str = f\"{total_critic_emissions:.2f}\" if total_critic_emissions > 0 else \"-\"\n",
    "    grand_total_energy_str = f\"{grand_total_energy:.2f}\" if grand_total_energy > 0 else \"-\"\n",
    "    grand_total_emissions_str = f\"{grand_total_emissions:.2f}\" if grand_total_emissions > 0 else \"-\"\n",
    "    \n",
    "    # Print totals\n",
    "    print(\"\\\\midrule\")\n",
    "    print(f\"Total & {total_idea_energy_str} & {total_idea_emissions_str} & {total_critic_energy_str} & {total_critic_emissions_str} & {grand_total_energy_str} & {grand_total_emissions_str} \\\\\\\\\")\n",
    "    print(\"\\\\bottomrule\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    # Close the resizebox command\n",
    "    print(\"}\")\n",
    "    print(\"\\\\label{tab:combined_emissions}\")\n",
    "    \n",
    "    # Add note about non-open source models in the table note\n",
    "    print(\"\\\\caption*{\\\\footnotesize{$^*$Non-open models: environmental impact values are rough estimates and may not be accurate.}}\")\n",
    "    \n",
    "    print(\"\\\\end{table}\")\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "\n",
    "combined_results = calculate_combined_emissions_table(\n",
    "    idea_token_counts=result, \n",
    "    critic_token_counts=critic_token_results, \n",
    "    model_params_dict=model_params_dict,\n",
    "    open_source_models=open_source_models\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liveideabench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
